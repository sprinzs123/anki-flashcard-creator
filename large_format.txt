"You are migrating a mission-critical application from your on-premises data centre to Google Cloud, and you need to ensure unhealthy compute instances within the autoscaled managed instances group are recreated automatically. What should you do?

-In the Instance Template, add a startup script that sends a heartbeat to the metadata server.
-Create a health check on port 443 and use that when creating the Managed Instance Group.
-In the Instance Template, add the label health-check.
-Select Multi-Zone instead of Single-Zone when creating the Managed Instance Group.
";"Create a health check on port 443 and use that when creating the Managed Instance Group.

Explanation
Our requirement is to ensure unhealthy VMs are recreated.

Select Multi-Zone instead of Single-Zone when creating the Managed Instance Group. is not right.
You can create two types of MIGs: A zonal MIG, which deploys instances to a single zone and a regional MIG, which deploys instances to multiple zones across the same region. However, this doesn't help with recreating unhealthy VMs.
Ref: https://cloud.google.com/compute/docs/instance-groups

In the Instance Template, add the label health-check. is not right.
Metadata entries are key-value pairs and do not influence any other behavior.
Ref: https://cloud.google.com/compute/docs/storing-retrieving-metadata

In the Instance Template, add a startup script that sends a heartbeat to the metadata server. is not right.
The startup script is executed only at the time of startup. Whereas we need something like a liveness check that monitors the status of the server periodically to identify if the VM is unhealthy. So this is not going to work.
Ref: https://cloud.google.com/compute/docs/startupscript

Create a health check on port 443 and use that when creating the Managed Instance Group. is the right answer.
To improve the availability of your application and to verify that your application is responding, you can configure an auto-healing policy for your managed instance group (MIG). An auto-healing policy relies on an application-based health check to verify that an application is responding as expected. If the auto healer determines that an application isn't responding, the managed instance group automatically recreates that instance. Since our application is a HTTPS web application, we need to set up our health check on port 443 which is the standard port for HTTPS.
Ref: https://cloud.google.com/compute/docs/instance-groups/autohealing-instances-in-migs"
"Your company wants to move all documents from a secure internal NAS drive to a Google Cloud Storage (GCS) bucket. The data contains personally identifiable information (PII) and sensitive customer information. Your company tax auditors need access to some of these documents. What security strategy would you recommend on GCS?

-Grant no Google Cloud Identity and Access Management (Cloud IAM) roles to users, and use granular ACLs on the bucket.
-Create randomized bucket and object names. Enable public access, but only provide specific file URLs to people who do not have Google accounts and need access.
-Use signed URLs to generate time-bound access to objects.
-Grant IAM read-only access to users, and use default ACLs on the bucket.
";"Grant no Google Cloud Identity and Access Management (Cloud IAM) roles to users, and use granular ACLs on the bucket.

Explanation
Use signed URLs to generate time-bound access to objects. is not right.
When dealing with sensitive customer information such as PII, using signed URLs is not a great idea as anyone with access to the URL has access to PII data. Signed URLs provide time-limited resource access to anyone in possession of the URL, regardless of whether they have a Google account. With PII Data, we want to be sure who has access and signed URLs don't guarantee that.
Ref: https://cloud.google.com/storage/docs/access-control/signed-urls

Grant IAM read-only access to users, and use default ACLs on the bucket. is not right.
We do not need to grant all IAM read-only access to this sensitive data. Just the users who need access to sensitive/PII data should be provided access to this data.

Create randomized bucket and object names. Enable public access, but only provide specific file URLs to people who do not have Google accounts and need access. is not right.
Enabling public access to the buckets and objects makes them visible to everyone. There are a number of scanning tools out in the market with the sole purpose of identifying buckets/objects that can be reached publicly. Should one of these tools be used by a bad actor to find out our public bucket/objects, it would result in a security breach.

Grant no Google Cloud Identity and Access Management (Cloud IAM) roles to users, and use granular ACLs on the bucket. is the right answer.
We start with no explicit access to any of the IAM users, and the bucket ACLs can then control which users can access what objects. This is the most secure way of ensuring just the people who require access to the bucket are provided with access. We block everyone from accessing the bucket and explicitly provided access to specific users through ACLs."
"Your colleague updated a deployment manager template of a production application serving live traffic. You want to deploy the update to the live environment later during the night when user traffic is at its lowest. The git diff on the pull request shows the changes are substantial and you would like to review the intended changes without applying the changes in the live environment. You want to do this in the most efficient way possible. What should you do?

-Execute the Deployment Manager template using the --preview option in the same project, and observe the status of interdependent resources.
-Use granular logging statements within the Deployment Manager template authored in Python.
-Monitor activity of the Deployment Manager executing on Stackdriver logging page of the GCP console.
-Execute the Deployment manager template against a separate project with the same configuration, and monitor for failures.
";"Execute the Deployment Manager template using the --preview option in the same project, and observe the status of interdependent resources.

Explanation
Requirements - confirm dependencies, rapid feedback.

Use granular logging statements within the Deployment Manager template authored in Python. is not right.
Deployment Manager doesn't provide the ability to set granular logging statements. Moreover, if that was possible the logging statements wouldn't be written to a log file until the template is applied and it is already too late as the template is applied and we haven't had a chance to confirm that the dependencies of all defined resources are properly met

Monitor activity of the Deployment Manager executing on Stackdriver logging page of the GCP console. is not right.
This doesn't give us a chance to confirm that the dependencies of all defined resources are properly met before executing it.

Execute the Deployment manager template against a separate project with the same configuration, and monitor for failures. is not right.
While we can identify whether dependencies are met by monitoring the failures, it is not rapid. We need rapid feedback on changes and we want that before changes are committed (i.e. applied) to the project

Execute the Deployment Manager template using the --preview option in the same project, and observe the status of interdependent resources. is the right answer.
After we have written a configuration file, we can preview the configuration before you create a deployment. Previewing a configuration lets you see the resources that Deployment Manager would create but does not actually instantiate any actual resources. In gcloud command-line, you use the create sub-command with the --preview flag to preview configuration changes.
Ref: https://cloud.google.com/deployment-manager"
"You want to migrate a mission-critical application from the on-premises data centre to Google Cloud Platform. Due to the mission-critical nature of the application, you want the application to scale up and scale down efficiently based on demand, but you do not wish to scale down any lower than 3 instances to ensure the application always has enough resources to handle sudden bursts in traffic. How should you configure the scaling to meet this requirement?

-Automatic Scaling with min_idle_instances set to 3.
-Manual Scaling with 3 instances.
-Basic Scaling with min_instances set to 3.
-Basic Scaling with max_instances set to 3.
";"Automatic Scaling with min_idle_instances set to 3.

Explanation
Manual Scaling with 3 instances. is not right.
Manual scaling uses resident instances that continuously run the specified number of instances regardless of the load level. This allows tasks such as complex initializations and applications that rely on the state of the memory over time. This does not autoscale based on the request rate so doesn't fit our requirements.
Ref: https://cloud.google.com/appengine/docs/standard/python/how-instances-are-managed

Basic Scaling with min_instances set to 3. is not right.
Basic scaling creates dynamic instances when your application receives requests. Each instance will be shut down when the app becomes idle. Basic scaling is ideal for work that is intermittent or driven by user activity. In absence of any load, the App engine may shut down all instances so it is not suitable for our requirement of "at least 3 instances at all times".
Ref: https://cloud.google.com/appengine/docs/standard/python/how-instances-are-managed

Basic Scaling with max_instances set to 3. is not right.
Basic scaling creates dynamic instances when your application receives requests. Each instance will be shut down when the app becomes idle. Basic scaling is ideal for work that is intermittent or driven by user activity. In absence of any load, the App engine may shut down all instances so it is not suitable for our requirement of "at least 3 instances at all times".
Ref: https://cloud.google.com/appengine/docs/standard/python/how-instances-are-managed

Automatic Scaling with min_idle_instances set to 3. is the right answer.
Automatic scaling creates dynamic instances based on request rate, response latencies, and other application metrics. However, if you specify the number of minimum idle instances, that specified number of instances run as resident instances while any additional instances are dynamic.
Ref: https://cloud.google.com/appengine/docs/standard/python/how-instances-are-managed"
